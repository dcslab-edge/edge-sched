//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21554848
// Cuda compilation tools, release 8.0, V8.0.61
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_61
.address_size 64

	// .globl	map

.visible .entry map(
	.param .u32 map_param_0,
	.param .u64 map_param_1,
	.param .u64 map_param_2,
	.param .u64 map_param_3,
	.param .u64 map_param_4,
	.param .u32 map_param_5
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<36>;
	.reg .f64 	%fd<56>;
	.reg .b64 	%rd<28>;


	ld.param.u32 	%r13, [map_param_0];
	ld.param.u64 	%rd18, [map_param_1];
	ld.param.u64 	%rd15, [map_param_2];
	ld.param.u64 	%rd16, [map_param_3];
	ld.param.u64 	%rd17, [map_param_4];
	ld.param.u32 	%r12, [map_param_5];
	cvta.to.global.u64 	%rd1, %rd18;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.s32	%p1, %r4, %r13;
	@%p1 bra 	BB0_10;

	mov.f64 	%fd54, 0d0000000000000000;
	setp.lt.s32	%p2, %r12, 1;
	@%p2 bra 	BB0_4;

	cvta.to.global.u64 	%rd25, %rd17;
	mul.lo.s32 	%r16, %r12, %r4;
	mul.wide.s32 	%rd19, %r16, 8;
	add.s64 	%rd24, %rd1, %rd19;
	mov.f64 	%fd54, 0d0000000000000000;
	mov.u32 	%r34, 0;

BB0_3:
	ld.global.f64 	%fd11, [%rd25];
	ld.global.f64 	%fd12, [%rd24];
	fma.rn.f64 	%fd54, %fd12, %fd11, %fd54;
	add.s64 	%rd25, %rd25, 8;
	add.s64 	%rd24, %rd24, 8;
	add.s32 	%r34, %r34, 1;
	setp.lt.s32	%p3, %r34, %r12;
	@%p3 bra 	BB0_3;

BB0_4:
	neg.f64 	%fd13, %fd54;
	mov.f64 	%fd14, 0d4338000000000000;
	mov.f64 	%fd15, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd16, %fd13, %fd15, %fd14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd16;
	}
	mov.f64 	%fd17, 0dC338000000000000;
	add.rn.f64 	%fd18, %fd16, %fd17;
	mov.f64 	%fd19, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd20, %fd18, %fd19, %fd13;
	mov.f64 	%fd21, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd22, %fd18, %fd21, %fd20;
	mov.f64 	%fd23, 0d3E928AF3FCA213EA;
	mov.f64 	%fd24, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3F81111111122322;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3FA55555555502A1;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	mov.f64 	%fd38, 0d3FC5555555555511;
	fma.rn.f64 	%fd39, %fd37, %fd22, %fd38;
	mov.f64 	%fd40, 0d3FE000000000000B;
	fma.rn.f64 	%fd41, %fd39, %fd22, %fd40;
	mov.f64 	%fd42, 0d3FF0000000000000;
	fma.rn.f64 	%fd43, %fd41, %fd22, %fd42;
	fma.rn.f64 	%fd44, %fd43, %fd22, %fd42;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r8, %temp}, %fd44;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd44;
	}
	shl.b32 	%r17, %r7, 20;
	add.s32 	%r18, %r9, %r17;
	mov.b64 	%fd55, {%r8, %r18};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd13;
	}
	mov.b32 	 %f2, %r19;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p4, %f1, 0f4086232B;
	@%p4 bra 	BB0_7;

	setp.gt.f64	%p5, %fd54, 0d8000000000000000;
	mov.f64 	%fd45, 0d7FF0000000000000;
	sub.f64 	%fd46, %fd45, %fd54;
	selp.f64	%fd55, 0d0000000000000000, %fd46, %p5;
	setp.geu.f32	%p6, %f1, 0f40874800;
	@%p6 bra 	BB0_7;

	shr.u32 	%r20, %r7, 31;
	add.s32 	%r21, %r7, %r20;
	shr.s32 	%r22, %r21, 1;
	shl.b32 	%r23, %r22, 20;
	add.s32 	%r24, %r23, %r9;
	mov.b64 	%fd47, {%r8, %r24};
	sub.s32 	%r25, %r7, %r22;
	shl.b32 	%r26, %r25, 20;
	add.s32 	%r27, %r26, 1072693248;
	mov.u32 	%r28, 0;
	mov.b64 	%fd48, {%r28, %r27};
	mul.f64 	%fd55, %fd47, %fd48;

BB0_7:
	@%p2 bra 	BB0_10;

	cvta.to.global.u64 	%rd20, %rd16;
	cvta.to.global.u64 	%rd21, %rd15;
	add.f64 	%fd49, %fd55, 0d3FF0000000000000;
	rcp.rn.f64 	%fd8, %fd49;
	mul.wide.s32 	%rd22, %r4, 8;
	add.s64 	%rd8, %rd21, %rd22;
	mul.lo.s32 	%r33, %r12, %r4;
	mul.wide.s32 	%rd23, %r33, 8;
	add.s64 	%rd27, %rd20, %rd23;
	add.s64 	%rd26, %rd1, %rd23;
	mov.u32 	%r35, 0;

BB0_9:
	ld.global.f64 	%fd50, [%rd8];
	sub.f64 	%fd51, %fd8, %fd50;
	ld.global.f64 	%fd52, [%rd26];
	mul.f64 	%fd53, %fd51, %fd52;
	st.global.f64 	[%rd27], %fd53;
	add.s64 	%rd27, %rd27, 8;
	add.s64 	%rd26, %rd26, 8;
	add.s32 	%r35, %r35, 1;
	setp.lt.s32	%p8, %r35, %r12;
	@%p8 bra 	BB0_9;

BB0_10:
	ret;
}

	// .globl	reducegrad
.visible .entry reducegrad(
	.param .u32 reducegrad_param_0,
	.param .u64 reducegrad_param_1,
	.param .u64 reducegrad_param_2,
	.param .u32 reducegrad_param_3
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<391>;
	.reg .f64 	%fd<99>;
	.reg .b64 	%rd<85>;


	ld.param.u32 	%r12, [reducegrad_param_0];
	ld.param.u64 	%rd33, [reducegrad_param_1];
	ld.param.u64 	%rd34, [reducegrad_param_2];
	ld.param.u32 	%r13, [reducegrad_param_3];
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r1, %r15, %r14, %r16;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB1_24;

	mov.u64 	%rd82, 0;
	setp.lt.s32	%p2, %r13, 4;
	@%p2 bra 	BB1_16;

	add.s32 	%r17, %r1, 16;
	shr.s32 	%r18, %r17, 31;
	shr.u32 	%r19, %r18, 27;
	add.s32 	%r20, %r17, %r19;
	and.b32  	%r21, %r20, -32;
	sub.s32 	%r2, %r17, %r21;
	add.s32 	%r22, %r1, 8;
	shr.s32 	%r23, %r22, 31;
	shr.u32 	%r24, %r23, 27;
	add.s32 	%r25, %r22, %r24;
	and.b32  	%r26, %r25, -32;
	sub.s32 	%r3, %r22, %r26;
	add.s32 	%r27, %r1, 4;
	shr.s32 	%r28, %r27, 31;
	shr.u32 	%r29, %r28, 27;
	add.s32 	%r30, %r27, %r29;
	and.b32  	%r31, %r30, -32;
	sub.s32 	%r4, %r27, %r31;
	add.s32 	%r32, %r1, 2;
	shr.s32 	%r33, %r32, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r32, %r34;
	and.b32  	%r36, %r35, -32;
	sub.s32 	%r5, %r32, %r36;
	add.s32 	%r37, %r1, 1;
	shr.s32 	%r38, %r37, 31;
	shr.u32 	%r39, %r38, 27;
	add.s32 	%r40, %r37, %r39;
	and.b32  	%r41, %r40, -32;
	sub.s32 	%r6, %r37, %r41;
	mov.u64 	%rd82, 0;

BB1_3:
	cvt.u64.u32	%rd77, %r1;
	cvt.s64.s32	%rd38, %r12;
	mov.f64 	%fd97, 0d0000000000000000;
	mov.f64 	%fd96, %fd97;
	mov.f64 	%fd95, %fd97;
	mov.f64 	%fd94, %fd97;
	setp.ge.s64	%p3, %rd77, %rd38;
	@%p3 bra 	BB1_6;

	cvta.to.global.u64 	%rd3, %rd33;
	mov.u32 	%r50, %nctaid.x;
	mul.lo.s32 	%r51, %r50, %r14;
	cvt.u64.u32	%rd4, %r51;
	mov.f64 	%fd97, 0d0000000000000000;
	mov.f64 	%fd96, %fd97;
	mov.f64 	%fd95, %fd97;
	mov.f64 	%fd94, %fd97;

BB1_5:
	cvt.s64.s32	%rd39, %r13;
	mul.lo.s64 	%rd40, %rd77, %rd39;
	add.s64 	%rd41, %rd40, %rd82;
	shl.b64 	%rd42, %rd41, 3;
	add.s64 	%rd43, %rd3, %rd42;
	ld.global.f64 	%fd28, [%rd43];
	add.f64 	%fd94, %fd94, %fd28;
	ld.global.f64 	%fd29, [%rd43+8];
	add.f64 	%fd95, %fd95, %fd29;
	ld.global.f64 	%fd30, [%rd43+16];
	add.f64 	%fd96, %fd96, %fd30;
	ld.global.f64 	%fd31, [%rd43+24];
	add.f64 	%fd97, %fd97, %fd31;
	add.s64 	%rd77, %rd4, %rd77;
	setp.lt.s64	%p4, %rd77, %rd38;
	@%p4 bra 	BB1_5;

BB1_6:
	and.b32  	%r293, %r16, 31;
	// inline asm
	mov.b64 {%r52,%r53}, %fd94;
	// inline asm
	// inline asm
	mov.b64 {%r54,%r55}, %fd95;
	// inline asm
	// inline asm
	mov.b64 {%r56,%r57}, %fd96;
	// inline asm
	// inline asm
	mov.b64 {%r58,%r59}, %fd97;
	// inline asm
	mov.u32 	%r283, 31;
	// inline asm
	shfl.idx.b32 %r60, %r52, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r64, %r53, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r68, %r54, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r72, %r55, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r76, %r56, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r80, %r57, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r84, %r58, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r88, %r59, %r2, %r283;
	// inline asm
	// inline asm
	mov.b64 %fd36, {%r60,%r64};
	// inline asm
	// inline asm
	mov.b64 %fd37, {%r68,%r72};
	// inline asm
	// inline asm
	mov.b64 %fd38, {%r76,%r80};
	// inline asm
	// inline asm
	mov.b64 %fd39, {%r84,%r88};
	// inline asm
	add.f64 	%fd40, %fd94, %fd36;
	add.f64 	%fd41, %fd95, %fd37;
	add.f64 	%fd42, %fd96, %fd38;
	add.f64 	%fd43, %fd97, %fd39;
	// inline asm
	mov.b64 {%r100,%r101}, %fd40;
	// inline asm
	// inline asm
	mov.b64 {%r102,%r103}, %fd41;
	// inline asm
	// inline asm
	mov.b64 {%r104,%r105}, %fd42;
	// inline asm
	// inline asm
	mov.b64 {%r106,%r107}, %fd43;
	// inline asm
	// inline asm
	shfl.idx.b32 %r108, %r100, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r112, %r101, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r116, %r102, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r120, %r103, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r124, %r104, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r128, %r105, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r132, %r106, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r136, %r107, %r3, %r283;
	// inline asm
	// inline asm
	mov.b64 %fd44, {%r108,%r112};
	// inline asm
	// inline asm
	mov.b64 %fd45, {%r116,%r120};
	// inline asm
	// inline asm
	mov.b64 %fd46, {%r124,%r128};
	// inline asm
	// inline asm
	mov.b64 %fd47, {%r132,%r136};
	// inline asm
	add.f64 	%fd48, %fd40, %fd44;
	add.f64 	%fd49, %fd41, %fd45;
	add.f64 	%fd50, %fd42, %fd46;
	add.f64 	%fd51, %fd43, %fd47;
	// inline asm
	mov.b64 {%r148,%r149}, %fd48;
	// inline asm
	// inline asm
	mov.b64 {%r150,%r151}, %fd49;
	// inline asm
	// inline asm
	mov.b64 {%r152,%r153}, %fd50;
	// inline asm
	// inline asm
	mov.b64 {%r154,%r155}, %fd51;
	// inline asm
	// inline asm
	shfl.idx.b32 %r156, %r148, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r160, %r149, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r164, %r150, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r168, %r151, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r172, %r152, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r176, %r153, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r180, %r154, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r184, %r155, %r4, %r283;
	// inline asm
	// inline asm
	mov.b64 %fd52, {%r156,%r160};
	// inline asm
	// inline asm
	mov.b64 %fd53, {%r164,%r168};
	// inline asm
	// inline asm
	mov.b64 %fd54, {%r172,%r176};
	// inline asm
	// inline asm
	mov.b64 %fd55, {%r180,%r184};
	// inline asm
	add.f64 	%fd56, %fd48, %fd52;
	add.f64 	%fd57, %fd49, %fd53;
	add.f64 	%fd58, %fd50, %fd54;
	add.f64 	%fd59, %fd51, %fd55;
	// inline asm
	mov.b64 {%r196,%r197}, %fd56;
	// inline asm
	// inline asm
	mov.b64 {%r198,%r199}, %fd57;
	// inline asm
	// inline asm
	mov.b64 {%r200,%r201}, %fd58;
	// inline asm
	// inline asm
	mov.b64 {%r202,%r203}, %fd59;
	// inline asm
	// inline asm
	shfl.idx.b32 %r204, %r196, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r208, %r197, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r212, %r198, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r216, %r199, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r220, %r200, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r224, %r201, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r228, %r202, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r232, %r203, %r5, %r283;
	// inline asm
	// inline asm
	mov.b64 %fd60, {%r204,%r208};
	// inline asm
	// inline asm
	mov.b64 %fd61, {%r212,%r216};
	// inline asm
	// inline asm
	mov.b64 %fd62, {%r220,%r224};
	// inline asm
	// inline asm
	mov.b64 %fd63, {%r228,%r232};
	// inline asm
	add.f64 	%fd64, %fd56, %fd60;
	add.f64 	%fd65, %fd57, %fd61;
	add.f64 	%fd66, %fd58, %fd62;
	add.f64 	%fd67, %fd59, %fd63;
	// inline asm
	mov.b64 {%r244,%r245}, %fd64;
	// inline asm
	// inline asm
	mov.b64 {%r246,%r247}, %fd65;
	// inline asm
	// inline asm
	mov.b64 {%r248,%r249}, %fd66;
	// inline asm
	// inline asm
	mov.b64 {%r250,%r251}, %fd67;
	// inline asm
	// inline asm
	shfl.idx.b32 %r252, %r244, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r256, %r245, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r260, %r246, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r264, %r247, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r268, %r248, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r272, %r249, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r276, %r250, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r280, %r251, %r6, %r283;
	// inline asm
	// inline asm
	mov.b64 %fd68, {%r252,%r256};
	// inline asm
	// inline asm
	mov.b64 %fd69, {%r260,%r264};
	// inline asm
	// inline asm
	mov.b64 %fd70, {%r268,%r272};
	// inline asm
	// inline asm
	mov.b64 %fd71, {%r276,%r280};
	// inline asm
	add.f64 	%fd13, %fd64, %fd68;
	add.f64 	%fd14, %fd65, %fd69;
	add.f64 	%fd15, %fd66, %fd70;
	add.f64 	%fd16, %fd67, %fd71;
	setp.ne.s32	%p5, %r293, 0;
	@%p5 bra 	BB1_15;

	cvta.to.global.u64 	%rd45, %rd34;
	shl.b64 	%rd46, %rd82, 3;
	add.s64 	%rd47, %rd45, %rd46;
	ld.global.u64 	%rd78, [%rd47];

BB1_8:
	mov.u64 	%rd8, %rd78;
	mov.b64 	 %fd72, %rd8;
	add.f64 	%fd73, %fd13, %fd72;
	mov.b64 	 %rd50, %fd73;
	atom.global.cas.b64 	%rd78, [%rd47], %rd8, %rd50;
	setp.ne.s64	%p6, %rd8, %rd78;
	@%p6 bra 	BB1_8;

	ld.global.u64 	%rd79, [%rd47+8];

BB1_10:
	mov.u64 	%rd12, %rd79;
	add.s64 	%rd54, %rd47, 8;
	mov.b64 	 %fd74, %rd12;
	add.f64 	%fd75, %fd14, %fd74;
	mov.b64 	 %rd55, %fd75;
	atom.global.cas.b64 	%rd79, [%rd54], %rd12, %rd55;
	setp.ne.s64	%p7, %rd12, %rd79;
	@%p7 bra 	BB1_10;

	ld.global.u64 	%rd80, [%rd47+16];

BB1_12:
	mov.u64 	%rd15, %rd80;
	add.s64 	%rd59, %rd47, 16;
	mov.b64 	 %fd76, %rd15;
	add.f64 	%fd77, %fd15, %fd76;
	mov.b64 	 %rd60, %fd77;
	atom.global.cas.b64 	%rd80, [%rd59], %rd15, %rd60;
	setp.ne.s64	%p8, %rd15, %rd80;
	@%p8 bra 	BB1_12;

	ld.global.u64 	%rd81, [%rd47+24];

BB1_14:
	mov.u64 	%rd18, %rd81;
	add.s64 	%rd64, %rd47, 24;
	mov.b64 	 %fd78, %rd18;
	add.f64 	%fd79, %fd16, %fd78;
	mov.b64 	 %rd65, %fd79;
	atom.global.cas.b64 	%rd81, [%rd64], %rd18, %rd65;
	setp.ne.s64	%p9, %rd18, %rd81;
	@%p9 bra 	BB1_14;

BB1_15:
	add.s64 	%rd82, %rd82, 4;
	cvt.s64.s32	%rd66, %r13;
	sub.s64 	%rd67, %rd66, %rd82;
	setp.gt.s64	%p10, %rd67, 3;
	@%p10 bra 	BB1_3;

BB1_16:
	cvt.s64.s32	%rd22, %r13;
	setp.ge.s64	%p11, %rd82, %rd22;
	@%p11 bra 	BB1_24;

	mov.u32 	%r298, %nctaid.x;
	mul.lo.s32 	%r299, %r298, %r14;
	cvt.u64.u32	%rd23, %r299;
	add.s32 	%r300, %r1, 16;
	shr.s32 	%r301, %r300, 31;
	shr.u32 	%r302, %r301, 27;
	add.s32 	%r303, %r300, %r302;
	and.b32  	%r304, %r303, -32;
	sub.s32 	%r7, %r300, %r304;
	add.s32 	%r305, %r1, 8;
	shr.s32 	%r306, %r305, 31;
	shr.u32 	%r307, %r306, 27;
	add.s32 	%r308, %r305, %r307;
	and.b32  	%r309, %r308, -32;
	sub.s32 	%r8, %r305, %r309;
	add.s32 	%r310, %r1, 4;
	shr.s32 	%r311, %r310, 31;
	shr.u32 	%r312, %r311, 27;
	add.s32 	%r313, %r310, %r312;
	and.b32  	%r314, %r313, -32;
	sub.s32 	%r9, %r310, %r314;
	add.s32 	%r315, %r1, 2;
	shr.s32 	%r316, %r315, 31;
	shr.u32 	%r317, %r316, 27;
	add.s32 	%r318, %r315, %r317;
	and.b32  	%r319, %r318, -32;
	sub.s32 	%r10, %r315, %r319;
	add.s32 	%r320, %r1, 1;
	shr.s32 	%r321, %r320, 31;
	shr.u32 	%r322, %r321, 27;
	add.s32 	%r323, %r320, %r322;
	and.b32  	%r324, %r323, -32;
	sub.s32 	%r11, %r320, %r324;

BB1_18:
	cvta.to.global.u64 	%rd68, %rd34;
	shl.b64 	%rd69, %rd82, 3;
	add.s64 	%rd25, %rd68, %rd69;
	cvt.s64.s32	%rd83, %r1;
	mov.f64 	%fd98, 0d0000000000000000;

BB1_19:
	mul.lo.s64 	%rd70, %rd83, %rd22;
	add.s64 	%rd71, %rd70, %rd82;
	cvta.to.global.u64 	%rd72, %rd33;
	shl.b64 	%rd73, %rd71, 3;
	add.s64 	%rd74, %rd72, %rd73;
	ld.global.f64 	%fd81, [%rd74];
	add.f64 	%fd98, %fd98, %fd81;
	cvt.s64.s32	%rd75, %r12;
	add.s64 	%rd83, %rd23, %rd83;
	setp.lt.s64	%p12, %rd83, %rd75;
	@%p12 bra 	BB1_19;

	and.b32  	%r390, %r16, 31;
	// inline asm
	mov.b64 {%r329,%r330}, %fd98;
	// inline asm
	mov.u32 	%r386, 31;
	// inline asm
	shfl.idx.b32 %r331, %r329, %r7, %r386;
	// inline asm
	// inline asm
	shfl.idx.b32 %r335, %r330, %r7, %r386;
	// inline asm
	// inline asm
	mov.b64 %fd83, {%r331,%r335};
	// inline asm
	add.f64 	%fd84, %fd98, %fd83;
	// inline asm
	mov.b64 {%r341,%r342}, %fd84;
	// inline asm
	// inline asm
	shfl.idx.b32 %r343, %r341, %r8, %r386;
	// inline asm
	// inline asm
	shfl.idx.b32 %r347, %r342, %r8, %r386;
	// inline asm
	// inline asm
	mov.b64 %fd85, {%r343,%r347};
	// inline asm
	add.f64 	%fd86, %fd84, %fd85;
	// inline asm
	mov.b64 {%r353,%r354}, %fd86;
	// inline asm
	// inline asm
	shfl.idx.b32 %r355, %r353, %r9, %r386;
	// inline asm
	// inline asm
	shfl.idx.b32 %r359, %r354, %r9, %r386;
	// inline asm
	// inline asm
	mov.b64 %fd87, {%r355,%r359};
	// inline asm
	add.f64 	%fd88, %fd86, %fd87;
	// inline asm
	mov.b64 {%r365,%r366}, %fd88;
	// inline asm
	// inline asm
	shfl.idx.b32 %r367, %r365, %r10, %r386;
	// inline asm
	// inline asm
	shfl.idx.b32 %r371, %r366, %r10, %r386;
	// inline asm
	// inline asm
	mov.b64 %fd89, {%r367,%r371};
	// inline asm
	add.f64 	%fd90, %fd88, %fd89;
	// inline asm
	mov.b64 {%r377,%r378}, %fd90;
	// inline asm
	// inline asm
	shfl.idx.b32 %r379, %r377, %r11, %r386;
	// inline asm
	// inline asm
	shfl.idx.b32 %r383, %r378, %r11, %r386;
	// inline asm
	// inline asm
	mov.b64 %fd91, {%r379,%r383};
	// inline asm
	add.f64 	%fd19, %fd90, %fd91;
	setp.ne.s32	%p13, %r390, 0;
	@%p13 bra 	BB1_23;

	ld.global.u64 	%rd84, [%rd25];

BB1_22:
	mov.u64 	%rd30, %rd84;
	mov.b64 	 %fd92, %rd30;
	add.f64 	%fd93, %fd19, %fd92;
	mov.b64 	 %rd76, %fd93;
	atom.global.cas.b64 	%rd84, [%rd25], %rd30, %rd76;
	setp.ne.s64	%p14, %rd30, %rd84;
	@%p14 bra 	BB1_22;

BB1_23:
	add.s64 	%rd82, %rd82, 1;
	setp.lt.s64	%p15, %rd82, %rd22;
	@%p15 bra 	BB1_18;

BB1_24:
	ret;
}


